{"origin_pdf_path": "https://arxiv.org/pdf/2308.08406", "text_in_pdf": "Content-based Recommendation Engine for Video Streaming Platform  \n\nPuskal  Khadka   \nDepartment of computer science   \nTexas intl college   \n(Tribhuvan University)   \nKathmandu, Nepal   \npuskalkumar.khadka@texasintl.edu.np   \nPrabhav Lamichhane   \nDepartment of computer science   \nTexas intl college   \n(Tribhuvan University)   \nKathmandu, Nepal   \nprabhav.lamichhane@texasintl.edu.np  \n\nAbstractâ€”  Recommendation  engine  suggest  content, product or services to the user by using machine learning algorithm. This  paper proposed a  content-based recommendation engine for providing video suggestions to the user based on their previous interests and choices. We will use TF-IDF text vectorization method to determine the relevance of words in a document. Then we will find out the similarity between each content by calculating cosine similarity between them. Finally, engine will recommend videos to the user based on the obtained similarity score value. In addition, we will measure the engineâ€™s performance by computing precision, recall, and F1 score of the proposed system.  \n\nKeywordsâ€”content-based recommendation engine; cosine similarity; machine learning; tf-idf  \n\nI. INTRODUCTION  \n\nVideo streaming platforms are drastically increased due to technological advancement and easy access of handheld devices in the recent years. Each platform consists of a huge amount of content of different genres, giving users a wide range of choices. Users are generally perplexed by the overwhelming content available on the platform so it is necessary to implement a proper recommendation engine. There are mainly three types of recommendation systems: collaborative filtering, content-based filtering, and hybrid recommendation system.  \n\nContent-based Filtering suggests results to the users based on their previous preferred interests and choices. It uses a machine learning algorithm to learn the similarity between different items and finally makes the prediction based on the similarity between them. This system generally creates a userâ€™s profile based on the types of items the user likes and then recommends by comparing items to the created user profile.  \n\nCollaborative filtering system collect and analyze data on userâ€™s behavior and predict results based on the similarity with other users. This system makes recommendations without having prior knowledge of the item to the current users. But if the item available in the system is itself new and also has not been rated by any users yet, then it will not be able to make the correct recommendations. This system has been used on many platforms such as Amazon [4], Netflix, and so on.  \n\nHybrid recommendation system is the combined form of both collaborative and content-based filtering. This system significantly undermines the weakness of the individual filtering  system.  They  make  predictions  by  taking consideration of usersâ€™ previous preferences as well as comparing them with other users having similar traits.  \n\nIn this research, we proposed content-based filtering for the design of a recommendation engine that predicts users' interest based on the feature of previous watch content such as cast, genre, and overview.  \n\nII. PROBLEM STATEMENT  \n\nVideo streaming platforms generally comprise a large amount of video content. In the maximum case, users are generally inclined towards a particular genre and there is a high tendency that users will consume similar types of content in the future. But without a recommendation engine on the platform, if the user wants to stream content of their own choices and preferences, each time they mostly have to manually search or filter content from the large pool of available videos. This process is generally tedious and repetitive. So, to solve this problem there is a need for an appropriate recommendation engine on the platform that provides accurate suggestions to the users based on their past watching choices and habits.  \n\nIII. RELATED WORK  \n\nDifferent types of recommendation engines have been researched and developed over the last few decades. They are widely adopted on several platforms such as Amazon, Netflix, Facebook, etc.  \n\nAmazon's recommender system [4] uses item-to-item collaborative filtering which predicts user interest on the basis of visitorsâ€™ recent purchase history and their browsing behavior, and ratings.  \n\nReference [12] introduces a group recommendation system  (GRS)  for  Facebook  using  a  combination  of hierarchical clustering technique and decision tree. Based on the number of experiment results, the author claims that the group  recommendation  system  makes $73\\%$ accurate recommendations.  \n\nG.  Shani  and  A.  Gunawardana  [7]  evaluate  the recommendation system by describing experimental settings appropriate  for  making choices  between  different recommendation algorithms. They review three types of experiments, starting with an offline setting, then reviewing user  studies,  and  finally  describing  large-scale  online experiments.  \n\nIV. METHODOLOGY  \n\nIn this section, we will discuss various steps involved in the designing of a proposed video recommender system in detail. First of all, we need to get datasets representing different videos. Then we have to preprocess and clean our raw  datasets.  After  that,  we  will  use  appropriate  text vectorizer method to find out the relevance of each word in the corpus. And finally, we need to use a similarity measuring algorithm to find out the similarity between videos, and then the system will recommend videos to the user based on the similarity score.  \n\nA. Data Gathering and Preprocessing  \n\nEfficiency of the recommendation system is highly dependent on the amount and quality of data available in the system. So, we took the data from Kaggle [9] which consists of 4803 movie datasets. This movie data will act as video content for our proposed video streaming platform. In this paper, from the original dataset, we used five moviesâ€™ data as sample video contents for showing the inner calculation and working of our recommendation engine from scratch. Each movie consists of several features such as genre, original language, release date, cast, overview, production company, and so on.  Out of this, we took three major features: Genre, Cast, and Overview for each movie. Then we create a separate table including mentioned three features as given below  \n\nTABLE II. SAMPLE MOVIE/VIDEO DATASETS   \n\nMovieGenreCastOverviewIronmanscifiRober Downey Jrmcu, weaponed suit, super heroTitanicromatceLeoanardoDicarpiosea disaster,romanceAvengersscifiRober Downey Jr, chris evansmcu, shield, super heroGreat Gatsbyromance,novelLeoanardo Dicarpiosocial difference,obsession,novelForrest GumpnovelTomHankinsipiration,lowiq,romance  \n\nAfter creating sample datasets, several data cleaning and preprocessing steps were carried out such as removing stop words,  null  values,  and  special  characters  from  each document. Then data from the three columns for each movie combine into a single filtered document corpus as given below  \n\nIronman: scifi robertdowneyjr mcu weaponedsuit superhero  \n\nTitanic: romance leonardodicarpio seadisaster romance  \n\nAvengers: sicfi roberdowneyjur chrisevans mcu shield superhero  \n\nGreat gatsby: romance novel leonardodicarpio socialdifference obsession novel  \n\nForrest gump: novel tomhank inspirational romance  \n\nB. Text Vectorization  \n\nAfter getting filter dataset, we have to convert this corpus into numerical representation i.e. in the form of vectors. This vector plays an important role in building a recommendation model in the later stage. There are several text  vectorization  techniques  [17]  in  natural  language processing such as Bag of Words, TF-IDF (Term Frequency and Inverse Document Frequency), Word2Vec, and so on. In this paper, we used TF-IDF text vectorization method to convert our filtered corpus into vectors. TF-IDF shows how important a word is to a document. It is obtained by computing two metrics: TF and IDF.  \n\n1) Term frequency $(T F)$ : Term frequency finds out the frequency of appearance of a particular term in the document. Suppose we have a document d, then term frequency of a word â€˜wâ€™ will be  \n\nTF $=$ ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ ğ‘œğ‘œğ‘œğ‘œ ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ â€²ğ‘¤ğ‘¤â€² ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ ğ‘–ğ‘–ğ‘–ğ‘– ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ â€²ğ‘‘ğ‘‘â€² ğ‘‡ğ‘‡ğ‘‡ğ‘‡ğ‘‡ğ‘‡ğ‘‡ğ‘‡ğ‘‡ğ‘‡ ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘› ğ‘œğ‘œğ‘œğ‘œ ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ ğ‘–ğ‘–ğ‘–ğ‘– ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ â€²ğ‘‘ğ‘‘â€²  \n\nUsing this formula, we calculate term frequency of each word in a corpus and obtained result is shown in the following table  \n\nTABLE II. TERM FREQUENCY OF WORD IN CORPUS   \n\nIronmanTitanicAvenger'sGreatgatsbyForrestgumpscifi1/50/41/60/60/4robertdowneyjr1/50/41/60/60/4mcu1/50/41/60/60/4weaponedsuit1/50/40/60/60/4superhero1/50/41/60/60/4romance0/52/40/61/61/4leonardodicarpio0/51/40/61/60/4seadisaster0/51/40/60/60/4chrisevans0/50/41/60/60/4shield0/50/41/60/60/4novel0/50/40/62/61/4socialdifference0/50/40/61/60/4obsession0/50/40/61/60/4tomhank0/50/40/60/61/4inspirational0/50/40/60/61/4  \n\n2)  Inverse document frequency $(I D F)$ : Higher the term frequency (TF), we can say that word has a significant meaning in the recommendation of the particular video. But this is not applicable in some cases such as stop words, or less meaningful words. For example, suppose a document has the word â€˜movieâ€™ four times. In that case, it doesnâ€™t mean the word â€˜movieâ€™ is significant while recommending because this word is less significant and doesnâ€™t play a vital role while recommending. So to overcome this issue, we have to calculate inverse document frequency.  \n\nIDF=ğ‘™ğ‘™ğ‘™ğ‘™ğ‘™ğ‘™ ğ‘‡ğ‘‡ğ‘‡ğ‘‡ğ‘‡ğ‘‡ğ‘‡ğ‘‡ğ‘‡ğ‘‡ ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘› ğ‘œğ‘œğ‘œğ‘œ ğ‘£ğ‘£ğ‘£ğ‘£ğ‘£ğ‘£ğ‘£ğ‘£ğ‘£ğ‘£ğ‘£ğ‘£ ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ ğ‘œğ‘œğ‘œğ‘œ ğ‘£ğ‘£ğ‘£ğ‘£ğ‘£ğ‘£ğ‘£ğ‘£ğ‘£ğ‘£ğ‘£ğ‘£ ğ‘¤ğ‘¤â„ğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œ ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ â€²ğ‘‘ğ‘‘â€² ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ â€²ğ‘¤ğ‘¤â€²  \n\nWe  take  logarithm  while  calculating  IDF  because sometimes the value of IDF without log is larger and more dominant over TF.  So, to negate its dominance and make it have the same effect as TF, we use a log while calculating IDF. By using the above formula, we calculate the IDF for each word in the corpus as follows  \n\nTABLE III. IDF OF EACH WORD IN CORPUS   \n\nIDFscifilog(5/2)robertdowneyjrlog(5/2)mcullog(5/2)weaponedsuitlog(5/1)superherolog(5/2)lomancelog(5/3)leonardodicarpiolog(5/2)seadisaster'log(5/1)chrisevanslog(5/1)shieldlog(5/1)novellog(5/2)socialdifferencelog(5/1)obsessionlog(5/1)tomhanklog(5/1)inspirationallog(5/1)  \n\n3) TF-IDF:   Word in the document is most important when it occurs frequently in its own document but rarely in other documents. Since TF gives how frequent or common a word is in its own document, and IDF gives how rare a word is in other documents so we need to multiply both terms to find the actual relevance of the word in the document. Table I and Table II show TF and IDF respectively, thus we calculate the value from both tables to get TF-IDF.  \n\n$$\n\\mathrm{TF-IDF}=\\mathrm{TF}\\mathrm{IDF}\n$$  \n\nTF-IDF value for each word is shown in the following table where each cell shows the multiplication of TF and IDF value.  \n\nTABLE IV. TF-IDF VALUE FOR EACH WORD   \n\nscifirobertdowneyjrmcuweaponedsuitsuperheroIronman(1/5)1og(5/2) =0.0795880(1/5)1og(5/2) =0.0795880(1/5)1og(5/2) =0.0795880(1/5)1og(5/1) =0.1397940(1/5)1og(5/2) =0.0795880Titanic(0/4)log(5/2ï¼‰ =0(0/4)1og(5/2) = 0.0(0/4)1og(5/2) =0(0/4)log(5/1) = 0.0(0/4)1og(5/2) =0Avengers(1/6)1og(5/2) =0.0795880(1/6)1og(5/2) =0.0663233(1/6)1og(5/2) = 0.0663233(0/6)1og(5/1) =0.0(1/6)1og(5/2) =0.0663233Greatgatsby(0/6)1og(5/2) =0(0/6)1og(5/2) = 0(0/6)1og(5/2) =0(0/6)1og(5/1) =0(0/6)1og(5/2) = 0Forrestgump(0/4)1og(5/2) =0(0/4)1og(5/2) =0(0/4)1og(5/2) =0(0/4)1og(5/1) =0(0/4)1og(5/2) =0  \n\nTABLE IV. (CONTINUED-1)   \n\nromanceleonardodicarpioseadisasterchrisevansshieldIronman(0/5)1og(5/3) = 0(0/5)1og(5/2) =0(0/5)1og(5/1) = 0(0/5)1og(5/1) = 0(0/5)log(5/1) = 0Titanic(2/4)log(5/3) =0.1109244(1/4)log(5/2) =0.0994850(1/4)log(5/1) =0.1747425(0/4)log(5/1) =0(0/4)1og(5/1) =0Avengers(0/6)log(5/3) = 0(0/6)1og(5/2) = 0.0(0/6)log(5/1) =0(1/6)log(5/1) =0.1164950(1/6)log(5/1) =0.1164950Greatgatsby(1/6)1og(5/3) =0.0369748(1/6)1og(5/2) =0.0663233(0/6)log(5/1) =0(0/6)1og(5/1) =0(0/6)log(5/1) =0Forrestgump(1/4)1og(5/3) =0.0554622(0/4)1og(5/2) =0(0/4)log(5/1) =0(0/4)1og(5/1) =0(0/4)1og(5/1) = 0  \n\nTABLE IV. (CONTINUED-2)   \n\nnovelsocialdifferenceobsessiontomhankinspirationalIronman(0/5)1og(5/2) = 0(0/5)1og(5/1) = 0(0/5)1og(5/1) =0(0/5)1og(5/1) = 0(0/5)1og(5/1) = 0Titanic(0/4)1og(5/2) = 0(0/4)log(5/1) = 0(0/4)log(5/1) =0(0/4)1og(5/1) = 0(0/4)log(5/1) = 0Avenger's(0/6)1og(5/2) =0(0/6)1og(5/1) =0(0/6)1og(5/1) =0(0/6)1og(5/1) =0(0/6)1og(5/1) =0Greatgatsby(2/6)1og(5/2) =0.1326467(1/6)log(5/1) =0.1164950(1/6)log(5/1) =0.1164950(0/6)log(5/1) = 0(0/6)1og(5/1) = 0Forrestgump(1/4)1og(5/2) =0.0994850(0/4)log(5/1) =0(0/4)log(5/1) =0(1/4)log(5/1) =0.1747425(1/4)log(5/1) =0.1747425  \n\nValue  in  each  cell  in  the  above  table  represents  the importance of a word in the document and each row in the table gives us a sequence of numbers. This sequence of numbers in order is known as numeric vector, which we will further compare to find the similarity [16] between them. The final combined sequence of values from each row of the TABLE IV which will act as a numeric vector for each movie is shown in the following table.  \n\nTABLE V. VECTOR FOR EACH MOVIE   \n\nIronmanTitanic0.0. ,0.0.0.0.1109244.0.0994850,0.1747425,0,0,0,0,0,0,0Avenger's0.0795880,0.0663233,0.0663233,0,0.0663233,0,0,0,0.1164950,0.1164950,0,0,0,0,0)Greatgatsbyï¼ˆ0,0,0,0,0,0.0369748,0.0663233,0,0,0,0.1326467,0.1164950,0.1164950,0,0)Forrestgump0.0.0,0.0.0.0554622,0.0.0.0,0.0994850.0.0.0.1747425.0.1747425  \n\nC. Cosine similarity  \n\nCosine similarity measures the similarity between two vectors by computing the cosine of the angle between them. Table $\\mathrm{V}$ shows the corresponding vectors for each movie. To calculate cosine similarity between them [18] we have to use the following formula.  \n\n$$\n\\cos(\\theta)={\\frac{\\mathbf{A\\cdotB}}{\\}}={\\frac{\\sum_{i=1}^{n}A_{i}B_{i}}{{\\sqrt{\\sum_{i=1}^{n}A_{i}^{2}}}{\\sqrt{\\sum_{i=1}^{n}B_{i}^{2}}}}}\n$$  \n\nwhere $\\mathrm{{A}_{i}}$ and $\\mathrm{B_{i}}$ are $\\mathbf{i}^{\\mathrm{th}}$ components of two vectors A and B respectively.  \n\nBy using the above formula, we calculate cosine similarly between every vector and obtained results which are plotted in the following heatmap diagram.  \n\n  \nFig. 1. Heatmap showing cosine similarity between movies  \n\nHere the range of cosine similarity lies between 0 and 1. Value 1 means the document is exactly similar to each other whereas 0 means two documents are completely dissimilar. In the diagonal, we can see that value is 1 because here both movies are the same movie.  \n\nNow we have a similarity score between each movie as shown in Fig. 1. Finally, our recommendation engine can suggest videos based on this similarity score. Before making suggestions, our engine will first look at the user's previous interests and history. Then at the time of auto-suggestion, this engine will traverse the cosine similarity value for the previously watched or searched movie and then suggest only those movies which have a maximum similarity score with the previous interest.  \n\nSuppose a user previously watched the movie â€˜Ironmanâ€™ on our video streaming platform. The next time, while suggesting, our system will look only for those movies, which have high cosine similarity with the movie â€˜Ironmanâ€™. If we look at our heatmap in the Fig. 1, â€˜Avengersâ€™ has the highest cosine similarity value with â€œIronmanâ€™ which is 0.48. So next time, our system will auto-recommend the movie â€˜Avengersâ€™ to the user. But it will not recommend movies such as â€˜Titanicâ€™, â€˜Great gatsbyâ€™ because if we look at the cosine similarity table in the above heatmap, the similarity score of movies â€˜Titanicâ€™, â€˜Great gatsbyâ€™ with the movie â€˜Ironmanâ€™ is zero.  \n\nIn this research paper, we calculate the precision, recall, and F1 score of our recommendation engine to evaluate its performance. For this, we select user â€˜Aâ€™ and make a separate report about his interest in the video content of the platform. Then we use our recommender to predict the set of videos in which user â€˜Aâ€™ may be interested based on his current search, history, and preferences. Finally, we compare these results with  userâ€™s  actual  liking  and  obtain  the  four  possible outcomes which are represented in the form of a confusion matrix as given below.  \n\nTABLE VI. CONFUSION MATRIX OF OUR RECOMMENDER   \n\nRecommendedNotRecommendedInterestedTrue positive (TP): 10False Negative (FN): 2Not InterestedFalse Positive (FP): 1True Negative (TN): 4  \n\nFrom the above table, we can see that, our engine made a total of 17 predictions for the user $\\mathbf{\\omega}_{\\mathbf{A}},$ . Out of this, our recommendation engine recommends 10 items/movies in which the user is actually interested i.e., true positive of our system is 10. Similarly, the total number of items in which user â€˜Aâ€™ is interested but not recommended by our system is 2 i.e., false negative is 2. There is a single item in which the user is not interested but recommended by our system i.e., false negative is 1. Finally, the total number of items in which user â€˜Aâ€™ is not interested and also not recommended by our system is 4 i.e., the true negative of our recommendation engine is 4. With the help of the confusion matrix, shown in TABLE VI, we can calculate the performance of our proposed system.  \n\nA. Precision  \n\nPrecision  gives  us  an  idea  about  out  of  all  true predictions, how many of them were actually true. It shows the  quality  of  the  correct  prediction  of  our  system. Mathematically, precision of a system can be defined as follows:  \n\n$$\n\\mathrm{Precision}=\\frac{T r u e\\:p o s i t i v e}{T r u e\\:p o s i t i v e\\:+F a l s e\\:p o s i t i v e}\n$$  \n\nIn our recommendation system on the sample dataset for a user $\\mathbf{\\omega}_{\\mathbf{A}},$ , the number of videos which is recommended on which user is actually interested i.e., TP is 10 and the total recommended videos are ${\\mathrm{TP}}{+}{\\mathrm{FP}}$ . Hence, the precision of our system will be  \n\n$$\n\\begin{array}{r l}{\\mathrm{Precision}={\\frac{10}{10+1}}}\\\\ {\\;}&{{}}\\\\ {=\\;0.90901}\\end{array}\n$$  \n\nSo, our video recommendation engine in this research paper  has  0.90901  precision  which  means  when  this recommender suggests video to the user, the percentage of time on which users are actually interested in that suggested video is approximately $90.90\\%$ .  \n\nB. Recall  \n\nRecall gives us an idea about out of all actual true results, how many of them were predicted as true. It can be defined as follow:  \n\n$$\n\\mathrm{Recall}=\\,\\frac{T r u e\\;P o s i t i v e}{T r u e\\;P o s i t i v e+F a l s e\\;N e g a t i v e}\n$$  \n\nIn our recommendation engine, for user â€˜Aâ€™, total number of videos on which user is actually interested i.e., sum of true positive and false positive is 12. But out of them, only 10 are recommended to the user. So, recall of our engine will be  \n\n$$\n\\begin{array}{l}{\\mathrm{Recall}=\\mathrm{\\frac{10}{10+2}}}\\\\ {=\\mathrm{0}.83333}\\end{array}\n$$  \n\nThus, recall of our system is 0.83333. This means our system correctly recommends $83.33\\%$ of videos in which the user is actually interested.  \n\n  \nFig. 2.  Venn diagram for Precision and recall terms  \n\nC. F1 Score  \n\nF1 score represents both precision and recall in a single metric and is calculated by taking the harmonic mean of precision and recall. It shows how many times our engine made a correct prediction across the entire dataset. Its value lies  between  0  to  1.  An  F1  score  of  1  means  all recommendations are $100\\%$ accurate. Mathematically, F1 score can be defined as follow  \n\n$$\n\\mathrm{F1~score}=\\begin{array}{l}{\\frac{2P r e c i s i o nR e c a l l}{P r e c i s i o n+R e c a l l}}\\end{array}\n$$  \n\nWe already calculate precision and recall of our system as seen in (1) and (2) respectively. Now we substitute this value in the above formula to get the F1 score of our system as follows  \n\n$$\n{\\begin{array}{r l}{{\\mathrm{F1~score}}=}&{{\\frac{20.909010.8333}{0.90901+0.83333}}}\\\\ &{}\\\\ {=}&{0.86952}\\end{array}}\n$$  \n\nSo F1 score of our proposed recommendation engine is 0.86952. This means, out of 1, the ability of our engine to correctly suggest a video to the user as per his previous preferences is 0.86952.  \n\nVI. CONCLUSION  \n\nThis research paper shows the adaptation of a contentbased recommendation engine for the video streaming platform.  Unlike  Collaborative  filtering,  content-based filtering doesnâ€™t need data from other users to recommend videos to the users. So, Content-based filtering approach becomes more useful for our system where users get recommendations on the basis of their own previous watch and search history. The preliminary experiment done on some selective users shows that our engine provides correct recommendations to the user.  \n\nIn the future, we will conduct experiments with a wide range of users having different tastes and watching traits to improve the accuracy level regarding the prediction of videos. Also, continuous research and development will be done in this engine to increase the efficiency level of our algorithm to provide a better user experience to the streaming platformâ€™s users.  \n\nACKNOWLEDGMENT  \n\nThis research was supported by Texas Imaginology, R&D Department of Texas international college. We are deeply  grateful  to  our  reviewers  for  their  review  and suggestion to enhance the final version of this research paper.  \n\nREFERENCES  \n\n[1]  G. Salton and M. J. McGill, Introduction to Modern Information Retrieval, McGraw-Hill, 1983.   \n[2]  G. Adomaviciu and A. Tuzhilin, \"Toward the next generation of recommender systems: a survey of the state-of-the-art and possible extensions,\" IEEE, vol. 17, no. 6, pp. 734-749, 2005.   \n[3] M. BalabanoviÄ‡ and Y. Shoham, \"Fab: Content-Based, Collaborative Recommendation,\" Commun. ACM, vol. 40, no. 3, pp. 66-72, 1997.   \n[4]  G. Linden, B. Smith and J. York, \"Amazon.com recommendations: item-to-item collaborative filtering,\" IEEE Internet Computing, vol. 7, no. 1, pp. 76-80, 2003.   \n[5]  N. Belkin and W. B. Croft, \"Information Filtering and Information Retrieval: Two Sides of the Same Coin?,\" Commun. ACM, vol. 35, no. 12, pp. 29-38, 1992.   \n[6]  M. J. Pazzani and D. Billsus, \"Content-Based Recommendation Systems,\" The Adaptive Web, vol. 4321, pp. 325-341, 2007.   \n[7]  G.  Shani  and  A.  Gunawardana,  \"Evaluating  Recommendation Systems,\" in Recommender Systems Handbook, Springer US, 2011, pp. 257-297.   \n[8]  J. Lu, D. Wu and W. W. G. Z. Mingsong Mao, \"Recommender system application developments: A survey,\" Decision Support Systems, vol. 74, pp. 12-32, 2015.   \n[9] \"Movie Dataset: Budgets, Genres, Insights,\" [Online]. Available: https://www.kaggle.com/datasets/utkarshx27/movies-dataset. [Accessed 3 March 2023].   \n[10] P.  Lops,  M.  Degemmis  and  G.  Semeraro,  \"Content-based Recommender Systems: State of the Art and Trends,\" in Recommender System Handbook, Springer US, 2011, pp. 73-105.   \n[11] S. Prakash, A. Nautiyal and M. Prasad, \"Machine Learning Algorithms for Recommender System - a comparative analysis,\" International Journal of Computer Applications Technology and Research, vol. 6, no. 2, pp. 97-100, 2017.   \n[12] E.-A.  Baatarjav,  S.  Phithakkitnukoon  and  R.  Dantu,  \"Group Recommendation  System  for  Facebook,\"  in  On  the  Move  to Meaningful Internet Systems: OTM 2008 Workshops, Springer Berlin Heidelberg, 2008, pp. 211-219.   \n[13] A. K. Singh and M. Shashi, \"Vectorization of Text Documents for Identifying  Unifiable  News  Articles,\"  International  Journal  of Advanced Computer Science and Applications, vol. 10, no. 7, 2019.   \n[14] R. Feldman and J. Sanger, The Text Mining Handbook: Advanced Approaches in Analyzing Unstructured Data, Cambridge University Press, 2007.   \n[15] \"Content-based  Filtering,\"  Google.com,  [Online].  Available: https://developers.google.com/machinelearning/recommendation/content-based/basics. [Accessed 5 March 2023].   \n[16] W. H. Gomaa and A. A. Fahmy, \"A Survey of Text Similarity Approaches,\" {International Journal of Computer Applications, vol. 68, no. 13, pp. 13-18, 2013.   \n[17] X. Yang, K. Yang, T. Cui, M. Chen and L. He, \"A Study of Text Vectorization  Method  Combining  Topic  Model  and  Transfer Learning,\" Processes, vol. 10, p. 350, 2022.   \n[18] G.-S. Victor, P. Antonia and S. Spyros, \"CSMR: A Scalable Algorithm for Text Clustering with Cosine Similarity and MapReduce,\" Springer, vol. 437, pp. 2\\`11-220, 2014.", "files_in_pdf": [{"path": ".pdf_temp/subset_1_10_9dd07ec1_1761753641/images/n96ue7.jpg", "size": 115251}, {"path": ".pdf_temp/subset_1_10_9dd07ec1_1761753641/images/25woe9.jpg", "size": 74139}, {"path": ".pdf_temp/subset_1_10_9dd07ec1_1761753641/images/wnqxar.jpg", "size": 38991}, {"path": ".pdf_temp/subset_1_10_9dd07ec1_1761753641/images/mw4s8b.jpg", "size": 77605}, {"path": ".pdf_temp/subset_1_10_9dd07ec1_1761753641/images/jhg1jj.jpg", "size": 52852}, {"path": ".pdf_temp/subset_1_10_9dd07ec1_1761753641/images/7qk2wj.jpg", "size": 75276}, {"path": ".pdf_temp/subset_1_10_9dd07ec1_1761753641/images/niiaxz.jpg", "size": 113962}, {"path": ".pdf_temp/subset_1_10_9dd07ec1_1761753641/images/5ihl2t.jpg", "size": 118151}, {"path": ".pdf_temp/subset_1_10_9dd07ec1_1761753641/images/nsovj6.jpg", "size": 116704}, {"path": ".pdf_temp/subset_1_10_9dd07ec1_1761753641/images/3jdx59.jpg", "size": 31380}]}